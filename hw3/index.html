<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Name: Ryan Arlett </div>

		<br>
		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-ryan_a/">https://cal-cs184-student.github.io/hw-webpages-ryan_a/</a>
		<br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-ryana_hw3">https://github.com/cal-cs184-student/sp25-hw3-ryana_hw3</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<p>The first step in ray tracing is to first generate the rays that come out of the camera. For each pixel, I generate a fixed number of rays (given by the -s parameter). For each ray, I first sample a random x,y position inside that pixel using a uniform random sampler, then scale these coordinates down to normalized image coordinates (0 to 1), and pass them to my camera->generate_ray function.</p>
		<p>This function converts the given coordinates into a direction for the new ray, then converts this direction from the camera’s local space to world coordinates using the camera’s transformation matrix. This is the direction for the new ray, and the rays origin is simply set at the camera’s world space position. I also set the ray’s min_t and max_t to the camera’s clipping distances. Now, we want to know what each of these rays is going to hit, which requires ray intersection functions for each primitive in the scene (triangles and spheres).</p>
		<p>For ray-triangle intersections, I used the Moller-Trumbore algorithm given in discussion, which allows me to efficiently calculate the intersection time as well as the barycentric coordinates of the hit location. After this, I check that the intersection time is valid (within the ray’s min_t and max_t), and also that the barycentric coordinates are valid so we know that the hit location is actually inside the triangle. If these are both true, I fill in the intersection struct to contain the needed information about the hit, like the time, surface normal, and the primitive it hit. For the normal vector, I use the barycentric coordinates to interpolate the triangle’s vertex normals, making sure to normalize the result as it could no longer be normalized after interpolation. I also reduce the ray’s max_t to the time of the intersection so that it can no longer intersect with primitives behind this one.</p>
		<p>My ray-sphere intersection function uses a quadratic equation to solve for the times of intersection. After first checking if this equation has any solutions (using the quadratic equation), I again check that the times are within the ray’s valid window. If so, I fill in the intersection struct and update the ray’s max_t similarly to before. With this done, my renderer can now draw simple scenes with normal shading. The images below are rendered at 360p with 1 sample per pixel.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p1/p1_spheres.png" width="480px"/>
				</td>
				<td style="text-align: center;">
				  <img src="p1/p1_cow.png" width="480px"/>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p1/p1_teapot.png" width="480px"/>
				</td>
				<td style="text-align: center;">
				  <img src="p1/p1_banana.png" width="480px"/>
				</td>
			  </tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<p>Though the scenes above are fairly simple - only having a few thousand triangles at most - they were still quite slow to render, as each ray had to perform an intersection test on every primitive in the scene in linear O(N) time. A much more efficient way to render is to organize primitives into bounding boxes such that a ray only needs to perform intersection tests with the primitives in the boxes that it intersects. By organizing these bounding boxes into a binary search tree, we can reduce the time complexity of the intersection testing to O(log(N)), a massive improvement when we have many triangles.</p>
		<p>My implementation generates the tree by recursively splitting groups of primitives in half until it reaches a small enough number of primitives per box. I always split along the longest axis of the total bounding box of the primitives to hopefully get the greatest reduction in box surface area. As for the splitting point, I originally went with the average of the primitive’s centroids along the splitting axis. I then sorted the primitives by swapping only the ones that needed to be swapped in a QuickSort like algorithm. While this implementation worked, I found this would sometimes result in heavily one sided trees, especially in the Cornell Box scenes, where the large outside box would stay intertwined with the model in the middle to quite some depth. Thus, I decided to split at the median instead to guarantee an evenly sided tree. To be able to find the median I first have to completely sort the primitives, for which I just used std::sort() to make my life easier. I then split in the middle of this now sorted list of primitives, recursively calling the function on both halves of the list.</p>
		<p>With the BVH working I could now render much more complicated scenes in a fraction of the time, and could significantly increase the number of rays per pixel to get smoother results. Some complicated models rendered at 360p with 64 samples per pixel are shown below.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p2/p2_beast.png" width="480px"/>
				</td>
				<td style="text-align: center;">
				  <img src="p2/p2_dragon.png" width="480px"/>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p2/p2_lucy.png" width="480px"/>
				</td>
				<td style="text-align: center;">
				  <img src="p2/p2_wall-e.png" width="480px"/>
				</td>
			  </tr>
			</table>
		</div>
		<p>For a quantitative look at the performance increase, when rendering the cow model (shown in part 1) at 480x360p with 1 sample per pixel, my renderer took 25.17s with no BVH acceleration and only 0.08s with the BVH. This huge performance increase is helped by the fact that the cow model can be divided quite well into bounding boxes due to its regular geometry. However, for simpler models like the Cornell Box spheres model (also shown in part 1), the BVH acceleration only improved the time from 4.43s to 3.54s when rendering both at 360p with 64 samples per pixel. This shows that for very simple geometry, as well as for models that cannot be divided up nicely like the large triangles of the Cornell Box, the BVH’s performance boost is less significant.</p>
		<h2>Part 3: Direct Illumination</h2>
		<p>Now that my renderer can quickly calculate ray intersections, I need a way to calculate the direct lighting at those intersection points. The mathematical answer to this question is the integral of all the incoming radiance over the hemisphere at that point, multiplied by the material’s bidirectional scattering distribution function (BSDF) to convert incoming radiance to outgoing radiance in the direction of the camera. However, since I cannot possibly solve this integral, I must estimate its answer with a Monte-Carlo estimator.</p>
		<p>The naive way to perform this estimate is to sample the incoming radiance uniformly over the hemisphere. My implementation does this by generating a randomly sampled direction for the new ray in the hemisphere, then converting it from local coordinates to world coordinates, then shooting the new ray in this direction. If this ray intersects the scene, I then get the incoming radiance from that direction using the emission property of the material it intersects. I then multiply by cos(θ) from Lambert's cosine law and divide by the probability of sampling a ray in that direction (which is just 1/2π since the sampling is uniform). Finally, I multiply by the BSDF to get the outgoing radiance to the camera. After taking the average of many of these estimated outgoing radiances, my Monte-Carlo estimate is complete and I am able to render direct lighting, as shown below. The L parameter gives how many of the new direct lighting sampling rays I use, and the S parameter is samples per pixel as before.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_bunny_H_S16_L8.png" width="640px"/>
				  <figcaption>Hemisphere Sampling: S=16 L=8</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_bunny_H_S64_L32.png" width="640px"/>
				  <figcaption>Hemisphere Sampling: S=64 L=32</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_spheres_H_S64_L32.png" width="640px"/>
				  <figcaption>Hemisphere Sampling: S=64 L=32</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		<p>As you can see, there is significant noise in these images, especially at lower L. This is because the chance of any light sampling ray hitting the area light is low, so only a few of the rays actually contribute anything to the outgoing radiance. This gives a lot of noise as there is a decent chance of not having any light rays hit the area light at all. From this, we can see that it makes much more sense to send all our lighting rays in the direction of lights instead of wasting them uniformly over the hemisphere where most of them will end up hitting an object with no emission. This gives the idea of light importance sampling, which I implement as follows:</p>
		<p>For every area light in the scene, I generate a L random rays pointing towards it, and also record the incoming radiance from the light in that direction and the probability of sampling a ray in that direction. Then, I perform an intersection test to ensure that no object lies in between the hit point and the light. After this, I can use the same formula as before to perform a Monte-Carlo estimate of the outgoing radiance reflected from that area light. For point lights, my logic is the same, however I only take one sample for them as there is no reason to take L samples as they would all lie in the same direction. After summing up the contributions from every light, my importance sampling implementation is complete. As you can see below, this drastically reduces the noise in the renders, and also allows me to render scenes with point lights.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_bunny_I_S16_L8.png" width="640px"/>
				  <figcaption>Importance Sampling: S=16 L=8</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_bunny_I_S64_L32.png" width="640px"/>
				  <figcaption>Importance Sampling: S=64 L=32</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_spheres_I_S64_L32.png" width="640px"/>
				  <figcaption>Importance Sampling: S=64 L=32</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_dragon_I_S16_L8.png" width="640px"/>
				  <figcaption>Importance Sampling: S=64 L=32</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_wall-e_I_S16_L8.png" width="640px"/>
				  <figcaption>Importance Sampling: S=64 L=32</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		<p>With importance sampling, at points where the entire area light is unobscured we would expect little noise even with a low L, as all the light rays will hit the light and get return similar radiances. However, at points such as soft shadows where the light is partially obscured, we get noise as some of the light rays are blocked while others aren’t. This requires L to be decently large to average this out and remove noise in soft shadows. As you can see in the images below, going up to L=64 is required to reduce noise in the soft shadow regions.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_spheres_I_S1_L1.png" width="640px"/>
				  <figcaption>Importance Sampling: S=1 L=1</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_spheres_I_S1_L4.png" width="640px"/>
				  <figcaption>Importance Sampling: S=1 L=4</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_spheres_I_S1_L16.png" width="640px"/>
				  <figcaption>Importance Sampling: S=1 L=16</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p3/p3_spheres_I_S1_L64.png" width="640px"/>
				  <figcaption>Importance Sampling: S=1 L=64</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		<p>Comparing these two implementations, importance sampling beats uniform sampling in every way. Since both are estimating the same integral, they have the same expected results at large L, but importance sampling is able to converge much quicker by only sampling from the rays that are going to contribute to the outgoing radiance (the ones going towards lights). This means importance sampling can produce the same images with a far lower L, which also makes it run much faster. Apart from the difference in noise, the only difference between the sets of images is that uniform sampling produces a glow around the light at the top of the Cornel Box while importance sampling doesn’t. This is just due to a slight difference in implementation where my uniform sampling implementation allows light to come out of the top side of the light, while importance sampling doesn’t.</p>
		<h2>Part 4: Global Illumination</h2>
		
		<h2>Part 5: Adaptive Sampling</h2>
		
		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>
